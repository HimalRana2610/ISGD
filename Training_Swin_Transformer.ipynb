{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "326d7e3c",
   "metadata": {},
   "source": [
    "# Multi-Attribute Facial Recognition Training Pipeline\n",
    "\n",
    "This notebook trains multiple backbone architectures for facial attribute prediction on the ISGD dataset (33 attributes, 320x320 images)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d6bae55e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "GPU: NVIDIA GeForce RTX 5050 Laptop GPU\n",
      "Memory: 7.96 GB\n"
     ]
    }
   ],
   "source": [
    "# Import Required Libraries\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, models\n",
    "import timm\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score, accuracy_score, f1_score, \n",
    "    precision_score, recall_score, confusion_matrix\n",
    ")\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d21b41a9",
   "metadata": {},
   "source": [
    "## Configuration - Select Your Backbone\n",
    "\n",
    "**Available Backbones:**\n",
    "- `convnext_tiny` - ConvNeXt Tiny (Efficient, modern CNN)\n",
    "- `convnext_base` - ConvNeXt Base (Larger ConvNeXt)\n",
    "- `resnet34` - ResNet34 (Lightweight classic)\n",
    "- `resnet50` - ResNet50 (Standard benchmark)\n",
    "- `resnext50_32x4d` - ResNeXt50 (Cardinality-based)\n",
    "- `resnext101_32x8d` - ResNeXt101 (Higher capacity)\n",
    "- `efficientnet_b0` - EfficientNet B0 (Efficient scaling)\n",
    "- `efficientnet_b2` - EfficientNet B2 (Better accuracy)\n",
    "- `mobilenetv2` - MobileNetV2 (Lightweight mobile)\n",
    "- `vit_base_patch16_224` - Vision Transformer Base (Transformer-based)\n",
    "- `swin_tiny_patch4_window7_224` - Swin Transformer Tiny (Hierarchical transformer)\n",
    "- `swin_base_patch4_window7_224` - Swin Transformer Base (Larger version)\n",
    "- `arcface_resnet50` - ArcFace with ResNet50 backbone (Face recognition specialized)\n",
    "- `adaface_resnet50` - AdaFace with ResNet50 backbone (Adaptive face recognition)\n",
    "- `regnety_016` / `regnetx_032` etc. - RegNet family (Design space optimized)\n",
    "- `replknet_31b` - RepLKNet (Large kernel CNN)\n",
    "- `inception_next_small` - InceptionNext (Hybrid inception-style)\n",
    "- `focalnet_tiny_srf` - FocalNet (Focal modulation)\n",
    "- `focalnet_base_lrf` - FocalNet Base\n",
    "\n",
    " > Tip: For RegNet you can choose any timm name starting with `regnety_` or `regnetx_`. For InceptionNext use names like `inception_next_tiny`, `inception_next_small`, `inception_next_base`. For RepLKNet, use available timm variants like `replknet_31b`. For FocalNet, use variants such as `focalnet_tiny_srf`, `focalnet_small_srf`, `focalnet_base_lrf`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fb68ce09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected Backbone: inception_next_tiny\n",
      "Training Configuration:\n",
      "  - Batch Size: 16\n",
      "  - Epochs: 3\n",
      "  - Learning Rate: 0.0001\n",
      "  - Image Size: 320x320\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# CONFIGURATION - CHANGE THIS TO SELECT BACKBONE\n",
    "# ============================================\n",
    "BACKBONE = 'inception_next_tiny'  # Change this to any backbone listed above\n",
    "\n",
    "# Training hyperparameters\n",
    "BATCH_SIZE = 16\n",
    "NUM_EPOCHS = 3\n",
    "LEARNING_RATE = 0.0001\n",
    "NUM_WORKERS = 0\n",
    "IMAGE_SIZE = 320\n",
    "VAL_SPLIT = 0.2\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "# Paths\n",
    "DATA_DIR = './ISGD'\n",
    "CSV_PATH = os.path.join(DATA_DIR, 'Attributes.csv')\n",
    "IMAGES_DIR = os.path.join(DATA_DIR, 'Images')\n",
    "MODEL_SAVE_DIR = './Models'\n",
    "RESULTS_DIR = './Results'\n",
    "\n",
    "# Create directories\n",
    "os.makedirs(MODEL_SAVE_DIR, exist_ok=True)\n",
    "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "\n",
    "print(f\"Selected Backbone: {BACKBONE}\")\n",
    "print(f\"Training Configuration:\")\n",
    "print(f\"  - Batch Size: {BATCH_SIZE}\")\n",
    "print(f\"  - Epochs: {NUM_EPOCHS}\")\n",
    "print(f\"  - Learning Rate: {LEARNING_RATE}\")\n",
    "print(f\"  - Image Size: {IMAGE_SIZE}x{IMAGE_SIZE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d7477ac",
   "metadata": {},
   "source": [
    "## Dataset Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a9cbf99a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n",
      "Dataset shape: (30141, 34)\n",
      "Number of attributes: 33\n",
      "\n",
      "Attributes: attractive, blurry_image, sharp_jawline, high_cheekbones, smiling, bald, receeding_hairline, long_hair, curly_hair, grey_hair, black_hair, has_beard, patchy_beard, has_mustache, well_groomed, has_makeup, wearing_glasses, wearing_hat, clear_skin, dark_circles, oily_skin, thick_eyebrow, big_eyes, big_lips, sharp_nose, adult, old, mouth_open, male, double_chin, veil, wrinkle, chubby\n",
      "\n",
      "Missing values after cleaning: 0\n",
      "\n",
      "Data types check:\n",
      "  All attributes are numeric: True\n",
      "\n",
      "Train set: 24112 samples\n",
      "Validation set: 6029 samples\n"
     ]
    }
   ],
   "source": [
    "# Custom Dataset Class\n",
    "class ISGDDataset(Dataset):\n",
    "    def __init__(self, df, images_dir, transform=None):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.images_dir = images_dir\n",
    "        self.transform = transform\n",
    "        \n",
    "        # Get attribute columns (all except image_id)\n",
    "        self.attributes = [col for col in df.columns if col != 'image_id']\n",
    "        self.num_attributes = len(self.attributes)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        img_name = row['image_id']\n",
    "        img_path = os.path.join(self.images_dir, img_name)\n",
    "        \n",
    "        # Load image\n",
    "        try:\n",
    "            image = Image.open(img_path).convert('RGB')\n",
    "        except:\n",
    "            # Handle different extensions\n",
    "            base_name = os.path.splitext(img_name)[0]\n",
    "            for ext in ['.jpg', '.jpeg', '.png']:\n",
    "                alt_path = os.path.join(self.images_dir, base_name + ext)\n",
    "                if os.path.exists(alt_path):\n",
    "                    image = Image.open(alt_path).convert('RGB')\n",
    "                    break\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        # Get labels for all attributes\n",
    "        # Convert to numeric and handle any remaining non-numeric values\n",
    "        label_values = pd.to_numeric(row[self.attributes], errors='coerce').fillna(0).astype(np.float32)\n",
    "        labels = torch.from_numpy(label_values.values)\n",
    "        \n",
    "        return image, labels\n",
    "\n",
    "# Load dataset\n",
    "print(\"Loading dataset...\")\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"Number of attributes: {len(df.columns) - 1}\")\n",
    "\n",
    "# Get attribute columns (all except image_id)\n",
    "attribute_cols = [col for col in df.columns if col != 'image_id']\n",
    "\n",
    "# Convert all attribute columns to numeric, handling any errors\n",
    "for col in attribute_cols:\n",
    "    df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "\n",
    "# Fill any NaN values with 0\n",
    "df[attribute_cols] = df[attribute_cols].fillna(0)\n",
    "\n",
    "# Convert to integers (0 or 1 for binary classification)\n",
    "df[attribute_cols] = df[attribute_cols].astype(int)\n",
    "\n",
    "print(f\"\\nAttributes: {', '.join(attribute_cols)}\")\n",
    "\n",
    "# Check for missing values\n",
    "print(f\"\\nMissing values after cleaning: {df.isnull().sum().sum()}\")\n",
    "\n",
    "# Verify data types\n",
    "print(f\"\\nData types check:\")\n",
    "print(f\"  All attributes are numeric: {df[attribute_cols].dtypes.apply(lambda x: x in ['int32', 'int64', 'float32', 'float64']).all()}\")\n",
    "\n",
    "# Split dataset\n",
    "train_df, val_df = train_test_split(df, test_size=VAL_SPLIT, random_state=RANDOM_SEED)\n",
    "print(f\"\\nTrain set: {len(train_df)} samples\")\n",
    "print(f\"Validation set: {len(val_df)} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f75941f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Computing attribute frequencies in training set...\n",
      "Positive counts per attribute before augmentation:\n",
      "attractive            11751\n",
      "blurry_image           1376\n",
      "sharp_jawline          6532\n",
      "high_cheekbones        9046\n",
      "smiling               11943\n",
      "bald                   1262\n",
      "receeding_hairline     1532\n",
      "long_hair              8102\n",
      "curly_hair              220\n",
      "grey_hair              1414\n",
      "black_hair            20416\n",
      "has_beard              6573\n",
      "patchy_beard            814\n",
      "has_mustache           8044\n",
      "well_groomed          17090\n",
      "has_makeup             7222\n",
      "wearing_glasses        2577\n",
      "wearing_hat            1935\n",
      "clear_skin            18470\n",
      "dark_circles           9728\n",
      "oily_skin              8297\n",
      "thick_eyebrow         15880\n",
      "big_eyes               5572\n",
      "big_lips                695\n",
      "sharp_nose             1631\n",
      "adult                 21534\n",
      "old                    1913\n",
      "mouth_open             9723\n",
      "male                  14189\n",
      "double_chin             381\n",
      "veil                    465\n",
      "wrinkle                5153\n",
      "chubby                 8931\n",
      "dtype: int64\n",
      "Augmenting attribute 'blurry_image' from 1376 to 3000 using 1376 base images...\n",
      "Augmenting attribute 'bald' from 1262 to 3000 using 1262 base images...\n",
      "Augmenting attribute 'receeding_hairline' from 1532 to 3000 using 1532 base images...\n",
      "Augmenting attribute 'curly_hair' from 220 to 3000 using 220 base images...\n",
      "Augmenting attribute 'grey_hair' from 1414 to 3000 using 1414 base images...\n",
      "Augmenting attribute 'patchy_beard' from 814 to 3000 using 814 base images...\n",
      "Augmenting attribute 'wearing_glasses' from 2577 to 3000 using 2577 base images...\n",
      "Augmenting attribute 'wearing_hat' from 1935 to 3000 using 1935 base images...\n",
      "Augmenting attribute 'big_lips' from 695 to 3000 using 695 base images...\n",
      "Augmenting attribute 'sharp_nose' from 1631 to 3000 using 1631 base images...\n",
      "Augmenting attribute 'old' from 1913 to 3000 using 1913 base images...\n",
      "Augmenting attribute 'double_chin' from 381 to 3000 using 381 base images...\n",
      "Augmenting attribute 'veil' from 465 to 3000 using 465 base images...\n",
      "Created 22785 augmented metadata rows.\n",
      "Balanced train set size: 46897 samples\n",
      "Train batches: 2932\n",
      "Validation batches: 377\n",
      "\n",
      "Training for 33 attributes\n"
     ]
    }
   ],
   "source": [
    "# Data Augmentation and Transforms\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "    transforms.RandomHorizontalFlip(p=1.0),  # compulsory horizontal flip\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Frequency-based balancing and augmentation\n",
    "MIN_POSITIVES = 3000\n",
    "print(\"\\nComputing attribute frequencies in training set...\")\n",
    "pos_counts = train_df[attribute_cols].sum(axis=0)\n",
    "print(\"Positive counts per attribute before augmentation:\")\n",
    "print(pos_counts)\n",
    "\n",
    "augmented_rows = []\n",
    "for attr in attribute_cols:\n",
    "    current_pos = int(pos_counts[attr])\n",
    "    if current_pos >= MIN_POSITIVES or current_pos == 0:\n",
    "        continue  # already enough or no positives to augment\n",
    "    needed = MIN_POSITIVES - current_pos\n",
    "    pos_samples = train_df[train_df[attr] == 1]\n",
    "    if len(pos_samples) == 0:\n",
    "        continue\n",
    "    print(f\"Augmenting attribute '{attr}' from {current_pos} to {MIN_POSITIVES} using {len(pos_samples)} base images...\")\n",
    "    # Repeat positive samples to reach at least `needed` new ones\n",
    "    reps = int(np.ceil(needed / len(pos_samples)))\n",
    "    pos_repeated = pd.concat([pos_samples] * reps, ignore_index=True).iloc[:needed]\n",
    "    # Mark that these will be augmented (images will be modified on-the-fly via transform)\n",
    "    augmented_rows.append(pos_repeated)\n",
    "\n",
    "if augmented_rows:\n",
    "    augmented_df = pd.concat(augmented_rows, ignore_index=True)\n",
    "    # Optionally shuffle to mix with originals\n",
    "    augmented_df = augmented_df.sample(frac=1.0, random_state=RANDOM_SEED).reset_index(drop=True)\n",
    "    print(f\"Created {len(augmented_df)} augmented metadata rows.\")\n",
    "    # Combine original training data with augmented copies\n",
    "    balanced_train_df = pd.concat([train_df, augmented_df], ignore_index=True)\n",
    "    balanced_train_df = balanced_train_df.sample(frac=1.0, random_state=RANDOM_SEED).reset_index(drop=True)\n",
    "else:\n",
    "    print(\"No augmentation needed based on MIN_POSITIVES setting.\")\n",
    "    balanced_train_df = train_df.copy()\n",
    "\n",
    "print(f\"Balanced train set size: {len(balanced_train_df)} samples\")\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = ISGDDataset(balanced_train_df, IMAGES_DIR, transform=train_transform)\n",
    "val_dataset = ISGDDataset(val_df, IMAGES_DIR, transform=val_transform)\n",
    "\n",
    "# Create dataloaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, \n",
    "                          shuffle=True, num_workers=NUM_WORKERS, pin_memory=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, \n",
    "                        shuffle=False, num_workers=NUM_WORKERS, pin_memory=True)\n",
    "\n",
    "print(f\"Train batches: {len(train_loader)}\")\n",
    "print(f\"Validation batches: {len(val_loader)}\")\n",
    "\n",
    "# Get number of attributes\n",
    "num_attributes = train_dataset.num_attributes\n",
    "print(f\"\\nTraining for {num_attributes} attributes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8a3d9e6",
   "metadata": {},
   "source": [
    "## Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "16241dfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Creating inception_next_tiny model...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters: 26,161,337\n",
      "Trainable parameters: 26,161,337\n"
     ]
    }
   ],
   "source": [
    "# ArcFace and AdaFace implementations\n",
    "class ArcFaceBackbone(nn.Module):\n",
    "    \"\"\"ArcFace backbone using ResNet50\"\"\"\n",
    "    def __init__(self, pretrained=True):\n",
    "        super(ArcFaceBackbone, self).__init__()\n",
    "        resnet = models.resnet50(pretrained=pretrained)\n",
    "        # Remove FC layer\n",
    "        self.features = nn.Sequential(*list(resnet.children())[:-1])\n",
    "        self.embedding_size = 512\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return x\n",
    "\n",
    "class AdaFaceBackbone(nn.Module):\n",
    "    \"\"\"AdaFace backbone using ResNet50 with adaptive margin\"\"\"\n",
    "    def __init__(self, pretrained=True):\n",
    "        super(AdaFaceBackbone, self).__init__()\n",
    "        resnet = models.resnet50(pretrained=pretrained)\n",
    "        # Remove FC layer and add adaptive feature extraction\n",
    "        self.features = nn.Sequential(*list(resnet.children())[:-1])\n",
    "        self.embedding_size = 512\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        # Normalize features (characteristic of AdaFace)\n",
    "        x = nn.functional.normalize(x, p=2, dim=1)\n",
    "        return x\n",
    "\n",
    "# Multi-Attribute Classification Model\n",
    "class MultiAttributeModel(nn.Module):\n",
    "    def __init__(self, backbone_name, num_attributes, pretrained=True):\n",
    "        super(MultiAttributeModel, self).__init__()\n",
    "        self.backbone_name = backbone_name\n",
    "        self.needs_pooling = False  # Flag for models that need spatial pooling\n",
    "        \n",
    "        # Load backbone based on selection\n",
    "        if 'convnext' in backbone_name:\n",
    "            self.backbone = timm.create_model(backbone_name, pretrained=pretrained)\n",
    "            in_features = self.backbone.head.fc.in_features\n",
    "            self.backbone.head.fc = nn.Identity()\n",
    "        \n",
    "        elif 'arcface' in backbone_name:\n",
    "            self.backbone = ArcFaceBackbone(pretrained=pretrained)\n",
    "            in_features = 2048  # ResNet50 feature dimension\n",
    "        \n",
    "        elif 'adaface' in backbone_name:\n",
    "            self.backbone = AdaFaceBackbone(pretrained=pretrained)\n",
    "            in_features = 2048  # ResNet50 feature dimension\n",
    "        \n",
    "        elif 'resnext' in backbone_name:\n",
    "            # Torchvision ResNeXt variants (e.g., resnext50_32x4d)\n",
    "            if hasattr(models, backbone_name):\n",
    "                self.backbone = getattr(models, backbone_name)(pretrained=pretrained)\n",
    "            else:\n",
    "                raise ValueError(f\"Unsupported ResNeXt variant: {backbone_name}\")\n",
    "            in_features = self.backbone.fc.in_features\n",
    "            self.backbone.fc = nn.Identity()\n",
    "        \n",
    "        elif 'resnet' in backbone_name:\n",
    "            if backbone_name == 'resnet34':\n",
    "                self.backbone = models.resnet34(pretrained=pretrained)\n",
    "            elif backbone_name == 'resnet50':\n",
    "                self.backbone = models.resnet50(pretrained=pretrained)\n",
    "            else:\n",
    "                raise ValueError(f\"Unsupported ResNet variant: {backbone_name}\")\n",
    "            in_features = self.backbone.fc.in_features\n",
    "            self.backbone.fc = nn.Identity()\n",
    "        \n",
    "        elif 'efficientnet' in backbone_name:\n",
    "            self.backbone = timm.create_model(backbone_name, pretrained=pretrained)\n",
    "            in_features = self.backbone.classifier.in_features\n",
    "            self.backbone.classifier = nn.Identity()\n",
    "        \n",
    "        elif 'mobilenetv2' in backbone_name:\n",
    "            self.backbone = models.mobilenet_v2(pretrained=pretrained)\n",
    "            in_features = self.backbone.classifier[1].in_features\n",
    "            self.backbone.classifier = nn.Identity()\n",
    "        \n",
    "        elif 'vit' in backbone_name:\n",
    "            self.backbone = timm.create_model(backbone_name, pretrained=pretrained, img_size=IMAGE_SIZE)\n",
    "            in_features = self.backbone.head.in_features\n",
    "            self.backbone.head = nn.Identity()\n",
    "        \n",
    "        elif 'swin' in backbone_name:\n",
    "            self.backbone = timm.create_model(backbone_name, pretrained=pretrained, img_size=IMAGE_SIZE)\n",
    "            in_features = self.backbone.head.in_features\n",
    "            self.backbone.head = nn.Identity()\n",
    "            # Swin Transformer needs spatial pooling when using non-standard image sizes\n",
    "            self.needs_pooling = True\n",
    "            self.global_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        \n",
    "        elif ('regnet' in backbone_name\n",
    "              or 'replknet' in backbone_name\n",
    "              or 'inception_next' in backbone_name\n",
    "              or 'focalnet' in backbone_name):\n",
    "            # Handle timm models with reset_classifier available\n",
    "            self.backbone = timm.create_model(backbone_name, pretrained=pretrained)\n",
    "            in_features = getattr(self.backbone, 'num_features', None)\n",
    "            if in_features is None:\n",
    "                # Fallback: attempt to access classifier/head\n",
    "                if hasattr(self.backbone, 'classifier') and hasattr(self.backbone.classifier, 'in_features'):\n",
    "                    in_features = self.backbone.classifier.in_features\n",
    "                elif hasattr(self.backbone, 'head') and hasattr(self.backbone.head, 'in_features'):\n",
    "                    in_features = self.backbone.head.in_features\n",
    "                else:\n",
    "                    raise ValueError(f\"Could not determine in_features for backbone {backbone_name}\")\n",
    "            if hasattr(self.backbone, 'reset_classifier'):\n",
    "                self.backbone.reset_classifier(0)\n",
    "            elif hasattr(self.backbone, 'classifier'):\n",
    "                self.backbone.classifier = nn.Identity()\n",
    "            elif hasattr(self.backbone, 'head'):\n",
    "                self.backbone.head = nn.Identity()\n",
    "        \n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported backbone: {backbone_name}\")\n",
    "        \n",
    "        # Classification head for multi-attribute prediction\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(in_features, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(512, num_attributes)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        features = self.backbone(x)\n",
    "        \n",
    "        # Apply global pooling if needed (for models with spatial dimensions)\n",
    "        if self.needs_pooling:\n",
    "            # Check if features have spatial dimensions [batch, H, W, channels] or [batch, channels, H, W]\n",
    "            if len(features.shape) == 4:\n",
    "                # Swin Transformer outputs [batch, H, W, channels] - permute to [batch, channels, H, W]\n",
    "                if features.shape[-1] > features.shape[1]:  # channels-last format\n",
    "                    features = features.permute(0, 3, 1, 2)\n",
    "                features = self.global_pool(features)\n",
    "                features = features.flatten(1)\n",
    "        \n",
    "        output = self.classifier(features)\n",
    "        return output\n",
    "\n",
    "# Create model\n",
    "print(f\"\\nCreating {BACKBONE} model...\")\n",
    "model = MultiAttributeModel(BACKBONE, num_attributes, pretrained=True)\n",
    "model = model.to(device)\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "321051cf",
   "metadata": {},
   "source": [
    "## Training Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d5c251c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training setup complete!\n"
     ]
    }
   ],
   "source": [
    "# Loss function and optimizer\n",
    "criterion = nn.BCEWithLogitsLoss()  # Binary Cross Entropy for multi-label classification\n",
    "optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=0.01)\n",
    "\n",
    "# Learning rate scheduler\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode='min', factor=0.5, patience=3\n",
    ")\n",
    "\n",
    "# Training history\n",
    "history = {\n",
    "    'train_loss': [],\n",
    "    'val_loss': [],\n",
    "    'val_acc': []\n",
    "}\n",
    "\n",
    "print(\"Training setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fe6c31b",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2e063632",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training functions defined!\n"
     ]
    }
   ],
   "source": [
    "# Training function\n",
    "def train_epoch(model, dataloader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    pbar = tqdm(dataloader, desc='Training')\n",
    "    for images, labels in pbar:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        pbar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
    "    \n",
    "    return running_loss / len(dataloader)\n",
    "\n",
    "# Validation function\n",
    "def validate_epoch(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        pbar = tqdm(dataloader, desc='Validation')\n",
    "        for images, labels in pbar:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            \n",
    "            # Store predictions and labels\n",
    "            preds = torch.sigmoid(outputs) > 0.5\n",
    "            all_preds.append(preds.cpu().numpy())\n",
    "            all_labels.append(labels.cpu().numpy())\n",
    "            \n",
    "            pbar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    all_preds = np.vstack(all_preds)\n",
    "    all_labels = np.vstack(all_labels)\n",
    "    accuracy = (all_preds == all_labels).mean()\n",
    "    \n",
    "    return running_loss / len(dataloader), accuracy\n",
    "\n",
    "print(\"Training functions defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f06f5823",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing data loading...\n",
      "✓ Successfully loaded test batch: torch.Size([16, 3, 320, 320]), torch.Size([16, 33])\n",
      "\n",
      "============================================================\n",
      "Starting Training: inception_next_tiny\n",
      "============================================================\n",
      "\n",
      "\n",
      "Epoch 1/3\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/2932 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (16x2304 and 768x512)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 25\u001b[39m\n\u001b[32m     22\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m-\u001b[39m\u001b[33m\"\u001b[39m * \u001b[32m60\u001b[39m)\n\u001b[32m     24\u001b[39m \u001b[38;5;66;03m# Train\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m train_loss = \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     27\u001b[39m \u001b[38;5;66;03m# Validate\u001b[39;00m\n\u001b[32m     28\u001b[39m val_loss, val_acc = validate_epoch(model, val_loader, criterion, device)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 13\u001b[39m, in \u001b[36mtrain_epoch\u001b[39m\u001b[34m(model, dataloader, criterion, optimizer, device)\u001b[39m\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[32m     12\u001b[39m optimizer.zero_grad()\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m outputs = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     14\u001b[39m loss = criterion(outputs, labels)\n\u001b[32m     16\u001b[39m \u001b[38;5;66;03m# Backward pass\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\SVNIT\\Semester - 5\\Machine Learning\\Project\\env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\SVNIT\\Semester - 5\\Machine Learning\\Project\\env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 142\u001b[39m, in \u001b[36mMultiAttributeModel.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    139\u001b[39m         features = \u001b[38;5;28mself\u001b[39m.global_pool(features)\n\u001b[32m    140\u001b[39m         features = features.flatten(\u001b[32m1\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m142\u001b[39m output = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mclassifier\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    143\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\SVNIT\\Semester - 5\\Machine Learning\\Project\\env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\SVNIT\\Semester - 5\\Machine Learning\\Project\\env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\SVNIT\\Semester - 5\\Machine Learning\\Project\\env\\Lib\\site-packages\\torch\\nn\\modules\\container.py:244\u001b[39m, in \u001b[36mSequential.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    242\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[32m    243\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m244\u001b[39m         \u001b[38;5;28minput\u001b[39m = \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    245\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\SVNIT\\Semester - 5\\Machine Learning\\Project\\env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\SVNIT\\Semester - 5\\Machine Learning\\Project\\env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\SVNIT\\Semester - 5\\Machine Learning\\Project\\env\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:125\u001b[39m, in \u001b[36mLinear.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    124\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m125\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mRuntimeError\u001b[39m: mat1 and mat2 shapes cannot be multiplied (16x2304 and 768x512)"
     ]
    }
   ],
   "source": [
    "# Test data loading first\n",
    "print(\"Testing data loading...\")\n",
    "try:\n",
    "    test_batch = next(iter(train_loader))\n",
    "    print(f\"✓ Successfully loaded test batch: {test_batch[0].shape}, {test_batch[1].shape}\")\n",
    "except Exception as e:\n",
    "    print(f\"✗ Error loading data: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    raise\n",
    "\n",
    "# Main training loop\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Starting Training: {BACKBONE}\")\n",
    "print(f\"{'='*60}\\n\")\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "best_epoch = 0\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    print(f\"\\nEpoch {epoch+1}/{NUM_EPOCHS}\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    # Train\n",
    "    train_loss = train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "    \n",
    "    # Validate\n",
    "    val_loss, val_acc = validate_epoch(model, val_loader, criterion, device)\n",
    "    \n",
    "    # Update learning rate\n",
    "    scheduler.step(val_loss)\n",
    "    \n",
    "    # Store history\n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['val_loss'].append(val_loss)\n",
    "    history['val_acc'].append(val_acc)\n",
    "    \n",
    "    # Print metrics\n",
    "    print(f\"\\nEpoch {epoch+1} Summary:\")\n",
    "    print(f\"  Train Loss: {train_loss:.4f}\")\n",
    "    print(f\"  Val Loss: {val_loss:.4f}\")\n",
    "    print(f\"  Val Accuracy: {val_acc:.4f}\")\n",
    "    print(f\"  Learning Rate: {optimizer.param_groups[0]['lr']:.6f}\")\n",
    "    \n",
    "    # Save best model\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        best_epoch = epoch + 1\n",
    "        model_path = os.path.join(MODEL_SAVE_DIR, f'{BACKBONE}_best.pth')\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'val_loss': val_loss,\n",
    "            'val_acc': val_acc,\n",
    "        }, model_path)\n",
    "        print(f\"  ✓ Best model saved! (Val Loss: {val_loss:.4f})\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Training Complete!\")\n",
    "print(f\"Best Epoch: {best_epoch}\")\n",
    "print(f\"Best Val Loss: {best_val_loss:.4f}\")\n",
    "print(f\"{'='*60}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd0fe5c2",
   "metadata": {},
   "source": [
    "## Load Best Model and Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f99a7f4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading best model for evaluation...\n",
      "Safe weights-only load failed: 'str' object has no attribute '__module__'\n",
      "Falling back to weights_only=False (only do this if the checkpoint is trusted).\n",
      "Checkpoint loaded with weights_only=False.\n",
      "Model ready for evaluation.\n",
      "Safe weights-only load failed: 'str' object has no attribute '__module__'\n",
      "Falling back to weights_only=False (only do this if the checkpoint is trusted).\n",
      "Checkpoint loaded with weights_only=False.\n",
      "Model ready for evaluation.\n"
     ]
    }
   ],
   "source": [
    "# Safe and compatible checkpoint loading for PyTorch >= 2.6\n",
    "import os\n",
    "import torch\n",
    "from torch.serialization import add_safe_globals, safe_globals\n",
    "\n",
    "print(\"Loading best model for evaluation...\")\n",
    "model_path = os.path.join(MODEL_SAVE_DIR, f\"{BACKBONE}_best.pth\")\n",
    "\n",
    "# 1) Prefer safe loading with weights_only=True, but allow numpy scalar class\n",
    "add_safe_globals([\"numpy._core.multiarray.scalar\"])  # allowlisted for trusted checkpoint\n",
    "\n",
    "checkpoint = None\n",
    "try:\n",
    "    # Use the safe globals context for extra safety\n",
    "    with safe_globals([\"numpy._core.multiarray.scalar\"]):\n",
    "        checkpoint = torch.load(model_path, map_location=device, weights_only=True)\n",
    "    print(\"Checkpoint loaded with weights_only=True.\")\n",
    "except Exception as e:\n",
    "    print(f\"Safe weights-only load failed: {e}\")\n",
    "    print(\"Falling back to weights_only=False (only do this if the checkpoint is trusted).\")\n",
    "    # 2) Fallback for older checkpoints saved with arbitrary pickled objects\n",
    "    checkpoint = torch.load(model_path, map_location=device, weights_only=False)\n",
    "    print(\"Checkpoint loaded with weights_only=False.\")\n",
    "\n",
    "# Support both plain state_dict and wrapped dict formats\n",
    "state_dict = checkpoint\n",
    "if isinstance(checkpoint, dict) and \"model_state_dict\" in checkpoint:\n",
    "    state_dict = checkpoint[\"model_state_dict\"]\n",
    "\n",
    "model.load_state_dict(state_dict)\n",
    "model.eval()\n",
    "print(\"Model ready for evaluation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b9b201f",
   "metadata": {},
   "source": [
    "## Per-Attribute Metrics Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c6a1c46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Calculating per-attribute metrics...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting: 100%|██████████| 377/377 [01:09<00:00,  5.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions shape: (6029, 33)\n",
      "Labels shape: (6029, 33)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Get predictions on validation set\n",
    "print(\"\\nCalculating per-attribute metrics...\")\n",
    "all_preds = []\n",
    "all_probs = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in tqdm(val_loader, desc='Predicting'):\n",
    "        images = images.to(device)\n",
    "        outputs = model(images)\n",
    "        \n",
    "        probs = torch.sigmoid(outputs)\n",
    "        preds = probs > 0.5\n",
    "        \n",
    "        all_probs.append(probs.cpu().numpy())\n",
    "        all_preds.append(preds.cpu().numpy())\n",
    "        all_labels.append(labels.cpu().numpy())\n",
    "\n",
    "# Concatenate all batches\n",
    "all_probs = np.vstack(all_probs)\n",
    "all_preds = np.vstack(all_preds)\n",
    "all_labels = np.vstack(all_labels)\n",
    "\n",
    "print(f\"Predictions shape: {all_preds.shape}\")\n",
    "print(f\"Labels shape: {all_labels.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcb1834b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Calculating metrics for each attribute...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing attributes: 100%|██████████| 33/33 [00:00<00:00, 158.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "OVERALL METRICS (Mean across all attributes)\n",
      "============================================================\n",
      "AUC                 : 0.9696\n",
      "ACCURACY            : 0.9392\n",
      "MACRO_F1            : 0.8768\n",
      "MICRO_F1            : 0.9392\n",
      "PRECISION           : 0.8300\n",
      "RECALL              : 0.8273\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Calculate metrics for each attribute\n",
    "attribute_names = train_dataset.attributes\n",
    "results = []\n",
    "\n",
    "print(\"\\nCalculating metrics for each attribute...\")\n",
    "for i, attr_name in enumerate(tqdm(attribute_names, desc='Processing attributes')):\n",
    "    y_true = all_labels[:, i]\n",
    "    y_pred = all_preds[:, i]\n",
    "    y_prob = all_probs[:, i]\n",
    "    \n",
    "    # Skip if only one class present\n",
    "    if len(np.unique(y_true)) < 2:\n",
    "        print(f\"Warning: Attribute '{attr_name}' has only one class in validation set\")\n",
    "        continue\n",
    "    \n",
    "    # Calculate metrics\n",
    "    try:\n",
    "        auc = roc_auc_score(y_true, y_prob)\n",
    "    except:\n",
    "        auc = 0.0\n",
    "    \n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    \n",
    "    # F1 scores\n",
    "    macro_f1 = f1_score(y_true, y_pred, average='macro', zero_division=0)\n",
    "    micro_f1 = f1_score(y_true, y_pred, average='micro', zero_division=0)\n",
    "    \n",
    "    # Precision and Recall\n",
    "    precision = precision_score(y_true, y_pred, average='binary', zero_division=0)\n",
    "    recall = recall_score(y_true, y_pred, average='binary', zero_division=0)\n",
    "    \n",
    "    results.append({\n",
    "        'attribute': attr_name,\n",
    "        'auc': auc,\n",
    "        'accuracy': accuracy,\n",
    "        'macro_f1': macro_f1,\n",
    "        'micro_f1': micro_f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall\n",
    "    })\n",
    "\n",
    "# Create results DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Calculate mean metrics\n",
    "mean_metrics = results_df[['auc', 'accuracy', 'macro_f1', 'micro_f1', 'precision', 'recall']].mean()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"OVERALL METRICS (Mean across all attributes)\")\n",
    "print(\"=\"*60)\n",
    "for metric, value in mean_metrics.items():\n",
    "    print(f\"{metric.upper():20s}: {value:.4f}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89df6510",
   "metadata": {},
   "source": [
    "## Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5cbc0a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Results saved to: ./Results\\swin_tiny_patch4_window7_224_metrics.csv\n",
      "\n",
      "Results preview:\n",
      "            attribute       auc  accuracy  macro_f1  micro_f1  precision  \\\n",
      "0          attractive  0.958628  0.883895  0.883810  0.883895   0.893772   \n",
      "1        blurry_image  0.983907  0.971139  0.878718  0.971139   0.698113   \n",
      "2       sharp_jawline  0.914605  0.855034  0.810249  0.855034   0.786572   \n",
      "3     high_cheekbones  0.984849  0.934981  0.928841  0.934981   0.910075   \n",
      "4             smiling  0.965246  0.898822  0.898596  0.898822   0.905148   \n",
      "5                bald  0.983498  0.976945  0.865972  0.976945   0.779923   \n",
      "6  receeding_hairline  0.968908  0.962515  0.814634  0.962515   0.743772   \n",
      "7           long_hair  0.985322  0.939625  0.932803  0.939625   0.905222   \n",
      "8          curly_hair  0.971311  0.992868  0.820513  0.992868   0.780000   \n",
      "9           grey_hair  0.993932  0.980262  0.906101  0.980262   0.865204   \n",
      "\n",
      "     recall  \n",
      "0  0.867944  \n",
      "1  0.865497  \n",
      "2  0.660534  \n",
      "3  0.905811  \n",
      "4  0.882737  \n",
      "5  0.711268  \n",
      "6  0.575758  \n",
      "7  0.917647  \n",
      "8  0.549296  \n",
      "9  0.784091  \n",
      "\n",
      "============================================================\n",
      "TOP 10 ATTRIBUTES (by Macro F1)\n",
      "============================================================\n",
      "      attribute  macro_f1  accuracy      auc\n",
      "wearing_glasses  0.975477  0.990546 0.994392\n",
      "           male  0.970695  0.971471 0.996480\n",
      "   has_mustache  0.955049  0.960358 0.992770\n",
      "      has_beard  0.953777  0.963676 0.993242\n",
      "     has_makeup  0.952141  0.959363 0.990183\n",
      "    wearing_hat  0.943782  0.981755 0.992208\n",
      "      long_hair  0.932803  0.939625 0.985322\n",
      "high_cheekbones  0.928841  0.934981 0.984849\n",
      "           veil  0.912157  0.993034 0.992949\n",
      "     mouth_open  0.908238  0.912921 0.967869\n",
      "\n",
      "============================================================\n",
      "BOTTOM 10 ATTRIBUTES (by Macro F1)\n",
      "============================================================\n",
      "         attribute  macro_f1  accuracy      auc\n",
      "          big_lips  0.719303  0.960524 0.927288\n",
      "      patchy_beard  0.748957  0.972798 0.948171\n",
      "             adult  0.774664  0.928346 0.942025\n",
      "       double_chin  0.774682  0.982418 0.972893\n",
      "     sharp_jawline  0.810249  0.855034 0.914605\n",
      "receeding_hairline  0.814634  0.962515 0.968908\n",
      "         oily_skin  0.818194  0.832310 0.909162\n",
      "        curly_hair  0.820513  0.992868 0.971311\n",
      "        sharp_nose  0.838683  0.948748 0.974494\n",
      "      well_groomed  0.840805  0.872450 0.936863\n"
     ]
    }
   ],
   "source": [
    "# Save results CSV\n",
    "csv_filename = f'{BACKBONE}_metrics.csv'\n",
    "csv_path = os.path.join(RESULTS_DIR, csv_filename)\n",
    "results_df.to_csv(csv_path, index=False)\n",
    "\n",
    "print(f\"\\n✓ Results saved to: {csv_path}\")\n",
    "print(f\"\\nResults preview:\")\n",
    "print(results_df.head(10))\n",
    "\n",
    "# Sort by F1 score and display top/bottom performing attributes\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TOP 10 ATTRIBUTES (by Macro F1)\")\n",
    "print(\"=\"*60)\n",
    "top_attrs = results_df.nlargest(10, 'macro_f1')[['attribute', 'macro_f1', 'accuracy', 'auc']]\n",
    "print(top_attrs.to_string(index=False))\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"BOTTOM 10 ATTRIBUTES (by Macro F1)\")\n",
    "print(\"=\"*60)\n",
    "bottom_attrs = results_df.nsmallest(10, 'macro_f1')[['attribute', 'macro_f1', 'accuracy', 'auc']]\n",
    "print(bottom_attrs.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4802bea2",
   "metadata": {},
   "source": [
    "## Training Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61f4c80a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "TRAINING COMPLETE - SUMMARY\n",
      "============================================================\n",
      "Backbone Model: swin_tiny_patch4_window7_224\n",
      "Total Parameters: 27,930,011\n",
      "Training Samples: 46897\n",
      "Validation Samples: 6029\n",
      "Number of Attributes: 33\n",
      "Number of Epochs: 3\n",
      "Best Epoch: 3\n",
      "\n",
      "Model saved at: ./Models\\swin_tiny_patch4_window7_224_best.pth\n",
      "Results saved at: ./Results\\swin_tiny_patch4_window7_224_metrics.csv\n",
      "\n",
      "============================================================\n",
      "MEAN METRICS\n",
      "============================================================\n",
      "AUC                 : 0.9696\n",
      "ACCURACY            : 0.9392\n",
      "MACRO_F1            : 0.8768\n",
      "MICRO_F1            : 0.9392\n",
      "PRECISION           : 0.8300\n",
      "RECALL              : 0.8273\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "ATTRIBUTE STATISTICS\n",
      "============================================================\n",
      "Total attributes evaluated: 33\n",
      "\n",
      "Metrics Range:\n",
      "  AUC: 0.9092 - 0.9965\n",
      "  Accuracy: 0.8323 - 0.9930\n",
      "  Macro F1: 0.7193 - 0.9755\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Final summary\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TRAINING COMPLETE - SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Backbone Model: {BACKBONE}\")\n",
    "print(f\"Total Parameters: {total_params:,}\")\n",
    "print(f\"Training Samples: {len(train_dataset)}\")\n",
    "print(f\"Validation Samples: {len(val_dataset)}\")\n",
    "print(f\"Number of Attributes: {num_attributes}\")\n",
    "print(f\"Number of Epochs: {NUM_EPOCHS}\")\n",
    "print(f\"Best Epoch: {best_epoch}\")\n",
    "print(f\"\\nModel saved at: {model_path}\")\n",
    "print(f\"Results saved at: {csv_path}\")\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"MEAN METRICS\")\n",
    "print(\"=\"*60)\n",
    "for metric, value in mean_metrics.items():\n",
    "    print(f\"{metric.upper():20s}: {value:.4f}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Display attribute distribution\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ATTRIBUTE STATISTICS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Total attributes evaluated: {len(results_df)}\")\n",
    "print(f\"\\nMetrics Range:\")\n",
    "print(f\"  AUC: {results_df['auc'].min():.4f} - {results_df['auc'].max():.4f}\")\n",
    "print(f\"  Accuracy: {results_df['accuracy'].min():.4f} - {results_df['accuracy'].max():.4f}\")\n",
    "print(f\"  Macro F1: {results_df['macro_f1'].min():.4f} - {results_df['macro_f1'].max():.4f}\")\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
