{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "326d7e3c",
   "metadata": {},
   "source": [
    "# Multi-Attribute Facial Recognition Training Pipeline\n",
    "\n",
    "This notebook trains multiple backbone architectures for facial attribute prediction on the ISGD dataset (33 attributes, 320x320 images)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d6bae55e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "GPU: NVIDIA GeForce RTX 5050 Laptop GPU\n",
      "Memory: 7.96 GB\n"
     ]
    }
   ],
   "source": [
    "# Import Required Libraries\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, models\n",
    "import timm\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score, accuracy_score, f1_score, \n",
    "    precision_score, recall_score, confusion_matrix\n",
    ")\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d21b41a9",
   "metadata": {},
   "source": [
    "## Configuration - Select Your Backbone\n",
    "\n",
    "**Available Backbones:**\n",
    "- `convnext_tiny` - ConvNeXt Tiny (Efficient, modern CNN)\n",
    "- `convnext_base` - ConvNeXt Base (Larger ConvNeXt)\n",
    "- `resnet34` - ResNet34 (Lightweight classic)\n",
    "- `resnet50` - ResNet50 (Standard benchmark)\n",
    "- `resnext50_32x4d` - ResNeXt50 (Cardinality-based)\n",
    "- `resnext101_32x8d` - ResNeXt101 (Higher capacity)\n",
    "- `efficientnet_b0` - EfficientNet B0 (Efficient scaling)\n",
    "- `efficientnet_b2` - EfficientNet B2 (Better accuracy)\n",
    "- `mobilenetv2` - MobileNetV2 (Lightweight mobile)\n",
    "- `vit_base_patch16_224` - Vision Transformer Base (Transformer-based)\n",
    "- `swin_tiny_patch4_window7_224` - Swin Transformer Tiny (Hierarchical transformer)\n",
    "- `swin_base_patch4_window7_224` - Swin Transformer Base (Larger version)\n",
    "- `arcface_resnet50` - ArcFace with ResNet50 backbone (Face recognition specialized)\n",
    "- `adaface_resnet50` - AdaFace with ResNet50 backbone (Adaptive face recognition)\n",
    "- `regnety_016` / `regnetx_032` etc. - RegNet family (Design space optimized)\n",
    "- `replknet_31b` - RepLKNet (Large kernel CNN)\n",
    "- `inception_next_small` - InceptionNext (Hybrid inception-style)\n",
    "- `focalnet_tiny_srf` - FocalNet (Focal modulation)\n",
    "- `focalnet_base_lrf` - FocalNet Base\n",
    "\n",
    " > Tip: For RegNet you can choose any timm name starting with `regnety_` or `regnetx_`. For InceptionNext use names like `inception_next_tiny`, `inception_next_small`, `inception_next_base`. For RepLKNet, use available timm variants like `replknet_31b`. For FocalNet, use variants such as `focalnet_tiny_srf`, `focalnet_small_srf`, `focalnet_base_lrf`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fb68ce09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected Backbone: convnext_tiny\n",
      "Training Configuration:\n",
      "  - Batch Size: 16\n",
      "  - Epochs: 3\n",
      "  - Learning Rate: 0.0001\n",
      "  - Image Size: 320x320\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# CONFIGURATION - CHANGE THIS TO SELECT BACKBONE\n",
    "# ============================================\n",
    "BACKBONE = 'convnext_tiny'  # Change this to any backbone listed above\n",
    "\n",
    "# Training hyperparameters\n",
    "BATCH_SIZE = 16\n",
    "NUM_EPOCHS = 3\n",
    "LEARNING_RATE = 0.0001\n",
    "NUM_WORKERS = 0\n",
    "IMAGE_SIZE = 320\n",
    "VAL_SPLIT = 0.2\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "# Paths\n",
    "DATA_DIR = './Data/CelebA'\n",
    "CSV_PATH = os.path.join(DATA_DIR, 'list_attr_celeba.csv')\n",
    "IMAGES_DIR = os.path.join(DATA_DIR, 'img_align_celeba')\n",
    "MODEL_SAVE_DIR = '.'\n",
    "RESULTS_DIR = '.'\n",
    "\n",
    "# Create directories\n",
    "os.makedirs(MODEL_SAVE_DIR, exist_ok=True)\n",
    "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "\n",
    "print(f\"Selected Backbone: {BACKBONE}\")\n",
    "print(f\"Training Configuration:\")\n",
    "print(f\"  - Batch Size: {BATCH_SIZE}\")\n",
    "print(f\"  - Epochs: {NUM_EPOCHS}\")\n",
    "print(f\"  - Learning Rate: {LEARNING_RATE}\")\n",
    "print(f\"  - Image Size: {IMAGE_SIZE}x{IMAGE_SIZE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d7477ac",
   "metadata": {},
   "source": [
    "## Dataset Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a9cbf99a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n",
      "Dataset shape: (202599, 41)\n",
      "Number of attributes: 40\n",
      "Dataset shape: (202599, 41)\n",
      "Number of attributes: 40\n",
      "\n",
      "Attributes: 5_o_Clock_Shadow, Arched_Eyebrows, Attractive, Bags_Under_Eyes, Bald, Bangs, Big_Lips, Big_Nose, Black_Hair, Blond_Hair, Blurry, Brown_Hair, Bushy_Eyebrows, Chubby, Double_Chin, Eyeglasses, Goatee, Gray_Hair, Heavy_Makeup, High_Cheekbones, Male, Mouth_Slightly_Open, Mustache, Narrow_Eyes, No_Beard, Oval_Face, Pale_Skin, Pointy_Nose, Receding_Hairline, Rosy_Cheeks, Sideburns, Smiling, Straight_Hair, Wavy_Hair, Wearing_Earrings, Wearing_Hat, Wearing_Lipstick, Wearing_Necklace, Wearing_Necktie, Young\n",
      "\n",
      "Missing values after cleaning: 0\n",
      "\n",
      "Data types check:\n",
      "  All attributes are numeric: True\n",
      "\n",
      "Sampled 40000 images from the dataset\n",
      "\n",
      "Train set: 32000 samples\n",
      "Validation set: 8000 samples\n",
      "\n",
      "Attributes: 5_o_Clock_Shadow, Arched_Eyebrows, Attractive, Bags_Under_Eyes, Bald, Bangs, Big_Lips, Big_Nose, Black_Hair, Blond_Hair, Blurry, Brown_Hair, Bushy_Eyebrows, Chubby, Double_Chin, Eyeglasses, Goatee, Gray_Hair, Heavy_Makeup, High_Cheekbones, Male, Mouth_Slightly_Open, Mustache, Narrow_Eyes, No_Beard, Oval_Face, Pale_Skin, Pointy_Nose, Receding_Hairline, Rosy_Cheeks, Sideburns, Smiling, Straight_Hair, Wavy_Hair, Wearing_Earrings, Wearing_Hat, Wearing_Lipstick, Wearing_Necklace, Wearing_Necktie, Young\n",
      "\n",
      "Missing values after cleaning: 0\n",
      "\n",
      "Data types check:\n",
      "  All attributes are numeric: True\n",
      "\n",
      "Sampled 40000 images from the dataset\n",
      "\n",
      "Train set: 32000 samples\n",
      "Validation set: 8000 samples\n"
     ]
    }
   ],
   "source": [
    "# Custom Dataset Class\n",
    "class ISGDDataset(Dataset):\n",
    "    def __init__(self, df, images_dir, transform=None):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.images_dir = images_dir\n",
    "        self.transform = transform\n",
    "        \n",
    "        # Get attribute columns (all except image_id)\n",
    "        self.attributes = [col for col in df.columns if col != 'image_id']\n",
    "        self.num_attributes = len(self.attributes)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        img_name = row['image_id']\n",
    "        img_path = os.path.join(self.images_dir, img_name)\n",
    "        \n",
    "        # Load image\n",
    "        try:\n",
    "            image = Image.open(img_path).convert('RGB')\n",
    "        except:\n",
    "            # Handle different extensions\n",
    "            base_name = os.path.splitext(img_name)[0]\n",
    "            for ext in ['.jpg', '.jpeg', '.png']:\n",
    "                alt_path = os.path.join(self.images_dir, base_name + ext)\n",
    "                if os.path.exists(alt_path):\n",
    "                    image = Image.open(alt_path).convert('RGB')\n",
    "                    break\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        # Get labels for all attributes\n",
    "        # Convert to numeric and handle any remaining non-numeric values\n",
    "        label_values = pd.to_numeric(row[self.attributes], errors='coerce').fillna(0).astype(np.float32)\n",
    "        labels = torch.from_numpy(label_values.values)\n",
    "        \n",
    "        return image, labels\n",
    "\n",
    "# Load dataset\n",
    "print(\"Loading dataset...\")\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"Number of attributes: {len(df.columns) - 1}\")\n",
    "\n",
    "# Get attribute columns (all except image_id)\n",
    "attribute_cols = [col for col in df.columns if col != 'image_id']\n",
    "\n",
    "# Convert all attribute columns to numeric, handling any errors\n",
    "for col in attribute_cols:\n",
    "    df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "\n",
    "# Fill any NaN values with 0\n",
    "df[attribute_cols] = df[attribute_cols].fillna(0)\n",
    "\n",
    "# Convert to integers (0 or 1 for binary classification)\n",
    "df[attribute_cols] = df[attribute_cols].astype(int)\n",
    "\n",
    "print(f\"\\nAttributes: {', '.join(attribute_cols)}\")\n",
    "\n",
    "# Check for missing values\n",
    "print(f\"\\nMissing values after cleaning: {df.isnull().sum().sum()}\")\n",
    "\n",
    "# Verify data types\n",
    "print(f\"\\nData types check:\")\n",
    "print(f\"  All attributes are numeric: {df[attribute_cols].dtypes.apply(lambda x: x in ['int32', 'int64', 'float32', 'float64']).all()}\")\n",
    "\n",
    "# Sample 40,000 images from the dataset\n",
    "SAMPLE_SIZE = 40000\n",
    "if len(df) > SAMPLE_SIZE:\n",
    "    df = df.sample(n=SAMPLE_SIZE, random_state=RANDOM_SEED).reset_index(drop=True)\n",
    "    print(f\"\\nSampled {SAMPLE_SIZE} images from the dataset\")\n",
    "else:\n",
    "    print(f\"\\nDataset has {len(df)} images (less than {SAMPLE_SIZE}), using entire dataset\")\n",
    "\n",
    "# Split dataset\n",
    "train_df, val_df = train_test_split(df, test_size=VAL_SPLIT, random_state=RANDOM_SEED)\n",
    "print(f\"\\nTrain set: {len(train_df)} samples\")\n",
    "print(f\"Validation set: {len(val_df)} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f75941f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train batches: 2000\n",
      "Validation batches: 500\n",
      "\n",
      "Training for 40 attributes\n"
     ]
    }
   ],
   "source": [
    "# Data Augmentation and Transforms\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = ISGDDataset(train_df, IMAGES_DIR, transform=train_transform)\n",
    "val_dataset = ISGDDataset(val_df, IMAGES_DIR, transform=val_transform)\n",
    "\n",
    "# Create dataloaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, \n",
    "                          shuffle=True, num_workers=NUM_WORKERS, pin_memory=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, \n",
    "                        shuffle=False, num_workers=NUM_WORKERS, pin_memory=True)\n",
    "\n",
    "print(f\"Train batches: {len(train_loader)}\")\n",
    "print(f\"Validation batches: {len(val_loader)}\")\n",
    "\n",
    "# Get number of attributes\n",
    "num_attributes = train_dataset.num_attributes\n",
    "print(f\"\\nTraining for {num_attributes} attributes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8a3d9e6",
   "metadata": {},
   "source": [
    "## Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "16241dfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Creating convnext_tiny model...\n",
      "Total parameters: 28,234,376\n",
      "Trainable parameters: 28,234,376\n",
      "Total parameters: 28,234,376\n",
      "Trainable parameters: 28,234,376\n"
     ]
    }
   ],
   "source": [
    "# ArcFace and AdaFace implementations\n",
    "class ArcFaceBackbone(nn.Module):\n",
    "    \"\"\"ArcFace backbone using ResNet50\"\"\"\n",
    "    def __init__(self, pretrained=True):\n",
    "        super(ArcFaceBackbone, self).__init__()\n",
    "        resnet = models.resnet50(pretrained=pretrained)\n",
    "        # Remove FC layer\n",
    "        self.features = nn.Sequential(*list(resnet.children())[:-1])\n",
    "        self.embedding_size = 512\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return x\n",
    "\n",
    "class AdaFaceBackbone(nn.Module):\n",
    "    \"\"\"AdaFace backbone using ResNet50 with adaptive margin\"\"\"\n",
    "    def __init__(self, pretrained=True):\n",
    "        super(AdaFaceBackbone, self).__init__()\n",
    "        resnet = models.resnet50(pretrained=pretrained)\n",
    "        # Remove FC layer and add adaptive feature extraction\n",
    "        self.features = nn.Sequential(*list(resnet.children())[:-1])\n",
    "        self.embedding_size = 512\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        # Normalize features (characteristic of AdaFace)\n",
    "        x = nn.functional.normalize(x, p=2, dim=1)\n",
    "        return x\n",
    "\n",
    "# Multi-Attribute Classification Model\n",
    "class MultiAttributeModel(nn.Module):\n",
    "    def __init__(self, backbone_name, num_attributes, pretrained=True):\n",
    "        super(MultiAttributeModel, self).__init__()\n",
    "        self.backbone_name = backbone_name\n",
    "        \n",
    "        # Load backbone based on selection\n",
    "        if 'convnext' in backbone_name:\n",
    "            self.backbone = timm.create_model(backbone_name, pretrained=pretrained)\n",
    "            in_features = self.backbone.head.fc.in_features\n",
    "            self.backbone.head.fc = nn.Identity()\n",
    "        \n",
    "        elif 'arcface' in backbone_name:\n",
    "            self.backbone = ArcFaceBackbone(pretrained=pretrained)\n",
    "            in_features = 2048  # ResNet50 feature dimension\n",
    "        \n",
    "        elif 'adaface' in backbone_name:\n",
    "            self.backbone = AdaFaceBackbone(pretrained=pretrained)\n",
    "            in_features = 2048  # ResNet50 feature dimension\n",
    "        \n",
    "        elif 'resnext' in backbone_name:\n",
    "            # Torchvision ResNeXt variants (e.g., resnext50_32x4d)\n",
    "            if hasattr(models, backbone_name):\n",
    "                self.backbone = getattr(models, backbone_name)(pretrained=pretrained)\n",
    "            else:\n",
    "                raise ValueError(f\"Unsupported ResNeXt variant: {backbone_name}\")\n",
    "            in_features = self.backbone.fc.in_features\n",
    "            self.backbone.fc = nn.Identity()\n",
    "        \n",
    "        elif 'resnet' in backbone_name:\n",
    "            if backbone_name == 'resnet34':\n",
    "                self.backbone = models.resnet34(pretrained=pretrained)\n",
    "            elif backbone_name == 'resnet50':\n",
    "                self.backbone = models.resnet50(pretrained=pretrained)\n",
    "            else:\n",
    "                raise ValueError(f\"Unsupported ResNet variant: {backbone_name}\")\n",
    "            in_features = self.backbone.fc.in_features\n",
    "            self.backbone.fc = nn.Identity()\n",
    "        \n",
    "        elif 'efficientnet' in backbone_name:\n",
    "            self.backbone = timm.create_model(backbone_name, pretrained=pretrained)\n",
    "            in_features = self.backbone.classifier.in_features\n",
    "            self.backbone.classifier = nn.Identity()\n",
    "        \n",
    "        elif 'mobilenetv2' in backbone_name:\n",
    "            self.backbone = models.mobilenet_v2(pretrained=pretrained)\n",
    "            in_features = self.backbone.classifier[1].in_features\n",
    "            self.backbone.classifier = nn.Identity()\n",
    "        \n",
    "        elif 'vit' in backbone_name:\n",
    "            self.backbone = timm.create_model(backbone_name, pretrained=pretrained, img_size=IMAGE_SIZE)\n",
    "            in_features = self.backbone.head.in_features\n",
    "            self.backbone.head = nn.Identity()\n",
    "        \n",
    "        elif 'swin' in backbone_name:\n",
    "            self.backbone = timm.create_model(backbone_name, pretrained=pretrained, img_size=IMAGE_SIZE)\n",
    "            in_features = self.backbone.head.in_features\n",
    "            self.backbone.head = nn.Identity()\n",
    "        \n",
    "        elif ('regnet' in backbone_name\n",
    "              or 'replknet' in backbone_name\n",
    "              or 'inception_next' in backbone_name\n",
    "              or 'focalnet' in backbone_name):\n",
    "            # Handle timm models with reset_classifier available\n",
    "            self.backbone = timm.create_model(backbone_name, pretrained=pretrained)\n",
    "            in_features = getattr(self.backbone, 'num_features', None)\n",
    "            if in_features is None:\n",
    "                # Fallback: attempt to access classifier/head\n",
    "                if hasattr(self.backbone, 'classifier') and hasattr(self.backbone.classifier, 'in_features'):\n",
    "                    in_features = self.backbone.classifier.in_features\n",
    "                elif hasattr(self.backbone, 'head') and hasattr(self.backbone.head, 'in_features'):\n",
    "                    in_features = self.backbone.head.in_features\n",
    "                else:\n",
    "                    raise ValueError(f\"Could not determine in_features for backbone {backbone_name}\")\n",
    "            if hasattr(self.backbone, 'reset_classifier'):\n",
    "                self.backbone.reset_classifier(0)\n",
    "            elif hasattr(self.backbone, 'classifier'):\n",
    "                self.backbone.classifier = nn.Identity()\n",
    "            elif hasattr(self.backbone, 'head'):\n",
    "                self.backbone.head = nn.Identity()\n",
    "        \n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported backbone: {backbone_name}\")\n",
    "        \n",
    "        # Classification head for multi-attribute prediction\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(in_features, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(512, num_attributes)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        features = self.backbone(x)\n",
    "        output = self.classifier(features)\n",
    "        return output\n",
    "\n",
    "# Create model\n",
    "print(f\"\\nCreating {BACKBONE} model...\")\n",
    "model = MultiAttributeModel(BACKBONE, num_attributes, pretrained=True)\n",
    "model = model.to(device)\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "321051cf",
   "metadata": {},
   "source": [
    "## Training Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d5c251c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training setup complete!\n"
     ]
    }
   ],
   "source": [
    "# Loss function and optimizer\n",
    "criterion = nn.BCEWithLogitsLoss()  # Binary Cross Entropy for multi-label classification\n",
    "optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=0.01)\n",
    "\n",
    "# Learning rate scheduler\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode='min', factor=0.5, patience=3\n",
    ")\n",
    "\n",
    "# Training history\n",
    "history = {\n",
    "    'train_loss': [],\n",
    "    'val_loss': [],\n",
    "    'val_acc': []\n",
    "}\n",
    "\n",
    "print(\"Training setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fe6c31b",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2e063632",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training functions defined!\n"
     ]
    }
   ],
   "source": [
    "# Training function\n",
    "def train_epoch(model, dataloader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    pbar = tqdm(dataloader, desc='Training')\n",
    "    for images, labels in pbar:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        pbar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
    "    \n",
    "    return running_loss / len(dataloader)\n",
    "\n",
    "# Validation function\n",
    "def validate_epoch(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        pbar = tqdm(dataloader, desc='Validation')\n",
    "        for images, labels in pbar:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            \n",
    "            # Store predictions and labels\n",
    "            preds = torch.sigmoid(outputs) > 0.5\n",
    "            all_preds.append(preds.cpu().numpy())\n",
    "            all_labels.append(labels.cpu().numpy())\n",
    "            \n",
    "            pbar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    all_preds = np.vstack(all_preds)\n",
    "    all_labels = np.vstack(all_labels)\n",
    "    accuracy = (all_preds == all_labels).mean()\n",
    "    \n",
    "    return running_loss / len(dataloader), accuracy\n",
    "\n",
    "print(\"Training functions defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f06f5823",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing data loading...\n",
      "✓ Successfully loaded test batch: torch.Size([16, 3, 320, 320]), torch.Size([16, 40])\n",
      "\n",
      "============================================================\n",
      "Starting Training: convnext_tiny\n",
      "============================================================\n",
      "\n",
      "\n",
      "Epoch 1/3\n",
      "------------------------------------------------------------\n",
      "✓ Successfully loaded test batch: torch.Size([16, 3, 320, 320]), torch.Size([16, 40])\n",
      "\n",
      "============================================================\n",
      "Starting Training: convnext_tiny\n",
      "============================================================\n",
      "\n",
      "\n",
      "Epoch 1/3\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   5%|▍         | 96/2000 [02:03<40:44,  1.28s/it, loss=-53.5511]  \n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 25\u001b[39m\n\u001b[32m     22\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m-\u001b[39m\u001b[33m\"\u001b[39m * \u001b[32m60\u001b[39m)\n\u001b[32m     24\u001b[39m \u001b[38;5;66;03m# Train\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m train_loss = \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     27\u001b[39m \u001b[38;5;66;03m# Validate\u001b[39;00m\n\u001b[32m     28\u001b[39m val_loss, val_acc = validate_epoch(model, val_loader, criterion, device)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 20\u001b[39m, in \u001b[36mtrain_epoch\u001b[39m\u001b[34m(model, dataloader, criterion, optimizer, device)\u001b[39m\n\u001b[32m     17\u001b[39m     loss.backward()\n\u001b[32m     18\u001b[39m     optimizer.step()\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m     running_loss += \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     21\u001b[39m     pbar.set_postfix({\u001b[33m'\u001b[39m\u001b[33mloss\u001b[39m\u001b[33m'\u001b[39m: \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss.item()\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m})\n\u001b[32m     23\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m running_loss / \u001b[38;5;28mlen\u001b[39m(dataloader)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Test data loading first\n",
    "print(\"Testing data loading...\")\n",
    "try:\n",
    "    test_batch = next(iter(train_loader))\n",
    "    print(f\"✓ Successfully loaded test batch: {test_batch[0].shape}, {test_batch[1].shape}\")\n",
    "except Exception as e:\n",
    "    print(f\"✗ Error loading data: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    raise\n",
    "\n",
    "# Main training loop\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Starting Training: {BACKBONE}\")\n",
    "print(f\"{'='*60}\\n\")\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "best_epoch = 0\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    print(f\"\\nEpoch {epoch+1}/{NUM_EPOCHS}\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    # Train\n",
    "    train_loss = train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "    \n",
    "    # Validate\n",
    "    val_loss, val_acc = validate_epoch(model, val_loader, criterion, device)\n",
    "    \n",
    "    # Update learning rate\n",
    "    scheduler.step(val_loss)\n",
    "    \n",
    "    # Store history\n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['val_loss'].append(val_loss)\n",
    "    history['val_acc'].append(val_acc)\n",
    "    \n",
    "    # Print metrics\n",
    "    print(f\"\\nEpoch {epoch+1} Summary:\")\n",
    "    print(f\"  Train Loss: {train_loss:.4f}\")\n",
    "    print(f\"  Val Loss: {val_loss:.4f}\")\n",
    "    print(f\"  Val Accuracy: {val_acc:.4f}\")\n",
    "    print(f\"  Learning Rate: {optimizer.param_groups[0]['lr']:.6f}\")\n",
    "    \n",
    "    # Save best model\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        best_epoch = epoch + 1\n",
    "        model_path = os.path.join(MODEL_SAVE_DIR, f'{BACKBONE}_best.pth')\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'val_loss': val_loss,\n",
    "            'val_acc': val_acc,\n",
    "        }, model_path)\n",
    "        print(f\"  ✓ Best model saved! (Val Loss: {val_loss:.4f})\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Training Complete!\")\n",
    "print(f\"Best Epoch: {best_epoch}\")\n",
    "print(f\"Best Val Loss: {best_val_loss:.4f}\")\n",
    "print(f\"{'='*60}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd0fe5c2",
   "metadata": {},
   "source": [
    "## Load Best Model and Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f99a7f4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading best model for evaluation...\n",
      "Safe weights-only load failed: 'str' object has no attribute '__module__'\n",
      "Falling back to weights_only=False (only do this if the checkpoint is trusted).\n",
      "Safe weights-only load failed: 'str' object has no attribute '__module__'\n",
      "Falling back to weights_only=False (only do this if the checkpoint is trusted).\n",
      "Checkpoint loaded with weights_only=False.\n",
      "Model ready for evaluation.\n",
      "Checkpoint loaded with weights_only=False.\n",
      "Model ready for evaluation.\n"
     ]
    }
   ],
   "source": [
    "# Safe and compatible checkpoint loading for PyTorch >= 2.6\n",
    "import os\n",
    "import torch\n",
    "from torch.serialization import add_safe_globals, safe_globals\n",
    "\n",
    "print(\"Loading best model for evaluation...\")\n",
    "model_path = os.path.join(MODEL_SAVE_DIR, f\"{BACKBONE}_best.pth\")\n",
    "\n",
    "# 1) Prefer safe loading with weights_only=True, but allow numpy scalar class\n",
    "add_safe_globals([\"numpy._core.multiarray.scalar\"])  # allowlisted for trusted checkpoint\n",
    "\n",
    "checkpoint = None\n",
    "try:\n",
    "    # Use the safe globals context for extra safety\n",
    "    with safe_globals([\"numpy._core.multiarray.scalar\"]):\n",
    "        checkpoint = torch.load(model_path, map_location=device, weights_only=True)\n",
    "    print(\"Checkpoint loaded with weights_only=True.\")\n",
    "except Exception as e:\n",
    "    print(f\"Safe weights-only load failed: {e}\")\n",
    "    print(\"Falling back to weights_only=False (only do this if the checkpoint is trusted).\")\n",
    "    # 2) Fallback for older checkpoints saved with arbitrary pickled objects\n",
    "    checkpoint = torch.load(model_path, map_location=device, weights_only=False)\n",
    "    print(\"Checkpoint loaded with weights_only=False.\")\n",
    "\n",
    "# Support both plain state_dict and wrapped dict formats\n",
    "state_dict = checkpoint\n",
    "if isinstance(checkpoint, dict) and \"model_state_dict\" in checkpoint:\n",
    "    state_dict = checkpoint[\"model_state_dict\"]\n",
    "\n",
    "model.load_state_dict(state_dict)\n",
    "model.eval()\n",
    "print(\"Model ready for evaluation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b9b201f",
   "metadata": {},
   "source": [
    "## Per-Attribute Metrics Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c6a1c46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Calculating per-attribute metrics...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting: 100%|██████████| 189/189 [00:57<00:00,  3.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions shape: (6029, 33)\n",
      "Labels shape: (6029, 33)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Get predictions on validation set\n",
    "print(\"\\nCalculating per-attribute metrics...\")\n",
    "all_preds = []\n",
    "all_probs = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in tqdm(val_loader, desc='Predicting'):\n",
    "        images = images.to(device)\n",
    "        outputs = model(images)\n",
    "        \n",
    "        probs = torch.sigmoid(outputs)\n",
    "        preds = probs > 0.5\n",
    "        \n",
    "        all_probs.append(probs.cpu().numpy())\n",
    "        all_preds.append(preds.cpu().numpy())\n",
    "        all_labels.append(labels.cpu().numpy())\n",
    "\n",
    "# Concatenate all batches\n",
    "all_probs = np.vstack(all_probs)\n",
    "all_preds = np.vstack(all_preds)\n",
    "all_labels = np.vstack(all_labels)\n",
    "\n",
    "print(f\"Predictions shape: {all_preds.shape}\")\n",
    "print(f\"Labels shape: {all_labels.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcb1834b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Calculating metrics for each attribute...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing attributes: 100%|██████████| 33/33 [00:00<00:00, 142.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "OVERALL METRICS (Mean across all attributes)\n",
      "============================================================\n",
      "AUC                 : 0.9576\n",
      "ACCURACY            : 0.9296\n",
      "MACRO_F1            : 0.8193\n",
      "MICRO_F1            : 0.9296\n",
      "PRECISION           : 0.8416\n",
      "RECALL              : 0.6914\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Calculate metrics for each attribute\n",
    "attribute_names = train_dataset.attributes\n",
    "results = []\n",
    "\n",
    "print(\"\\nCalculating metrics for each attribute...\")\n",
    "for i, attr_name in enumerate(tqdm(attribute_names, desc='Processing attributes')):\n",
    "    y_true = all_labels[:, i]\n",
    "    y_pred = all_preds[:, i]\n",
    "    y_prob = all_probs[:, i]\n",
    "    \n",
    "    # Skip if only one class present\n",
    "    if len(np.unique(y_true)) < 2:\n",
    "        print(f\"Warning: Attribute '{attr_name}' has only one class in validation set\")\n",
    "        continue\n",
    "    \n",
    "    # Calculate metrics\n",
    "    try:\n",
    "        auc = roc_auc_score(y_true, y_prob)\n",
    "    except:\n",
    "        auc = 0.0\n",
    "    \n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    \n",
    "    # F1 scores\n",
    "    macro_f1 = f1_score(y_true, y_pred, average='macro', zero_division=0)\n",
    "    micro_f1 = f1_score(y_true, y_pred, average='micro', zero_division=0)\n",
    "    \n",
    "    # Precision and Recall\n",
    "    precision = precision_score(y_true, y_pred, average='binary', zero_division=0)\n",
    "    recall = recall_score(y_true, y_pred, average='binary', zero_division=0)\n",
    "    \n",
    "    results.append({\n",
    "        'attribute': attr_name,\n",
    "        'auc': auc,\n",
    "        'accuracy': accuracy,\n",
    "        'macro_f1': macro_f1,\n",
    "        'micro_f1': micro_f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall\n",
    "    })\n",
    "\n",
    "# Create results DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Calculate mean metrics\n",
    "mean_metrics = results_df[['auc', 'accuracy', 'macro_f1', 'micro_f1', 'precision', 'recall']].mean()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"OVERALL METRICS (Mean across all attributes)\")\n",
    "print(\"=\"*60)\n",
    "for metric, value in mean_metrics.items():\n",
    "    print(f\"{metric.upper():20s}: {value:.4f}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89df6510",
   "metadata": {},
   "source": [
    "## Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5cbc0a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Results saved to: ./Results\\convnext_tiny_metrics.csv\n",
      "\n",
      "Results preview:\n",
      "            attribute       auc  accuracy  macro_f1  micro_f1  precision  \\\n",
      "0          attractive  0.944721  0.863327  0.862944  0.863327   0.893563   \n",
      "1        blurry_image  0.974758  0.962846  0.846112  0.962846   0.635321   \n",
      "2       sharp_jawline  0.890870  0.838116  0.776423  0.838116   0.801189   \n",
      "3     high_cheekbones  0.977561  0.918229  0.911538  0.918229   0.866786   \n",
      "4             smiling  0.959535  0.877923  0.877911  0.877923   0.841295   \n",
      "5                bald  0.978549  0.976281  0.838737  0.976281   0.898305   \n",
      "6  receeding_hairline  0.953015  0.953392  0.712881  0.953392   0.777027   \n",
      "7           long_hair  0.985305  0.936971  0.928521  0.936971   0.933194   \n",
      "8          curly_hair  0.929956  0.988555  0.524519  0.988555   1.000000   \n",
      "9           grey_hair  0.992960  0.978438  0.886353  0.978438   0.944000   \n",
      "\n",
      "     recall  \n",
      "0  0.820901  \n",
      "1  0.809942  \n",
      "2  0.559644  \n",
      "3  0.908622  \n",
      "4  0.920564  \n",
      "5  0.559859  \n",
      "6  0.316804  \n",
      "7  0.876471  \n",
      "8  0.028169  \n",
      "9  0.670455  \n",
      "\n",
      "============================================================\n",
      "TOP 10 ATTRIBUTES (by Macro F1)\n",
      "============================================================\n",
      "      attribute  macro_f1  accuracy      auc\n",
      "wearing_glasses  0.974442  0.990380 0.996162\n",
      "           male  0.966165  0.967325 0.996344\n",
      "      has_beard  0.953937  0.963012 0.992552\n",
      "   has_mustache  0.948737  0.954055 0.991961\n",
      "    wearing_hat  0.945552  0.982418 0.992017\n",
      "     has_makeup  0.936282  0.947753 0.987360\n",
      "      long_hair  0.928521  0.936971 0.985305\n",
      "high_cheekbones  0.911538  0.918229 0.977561\n",
      "         chubby  0.889823  0.898491 0.965747\n",
      "        wrinkle  0.886691  0.918892 0.972545\n",
      "\n",
      "============================================================\n",
      "BOTTOM 10 ATTRIBUTES (by Macro F1)\n",
      "============================================================\n",
      "         attribute  macro_f1  accuracy      auc\n",
      "       double_chin  0.496240  0.985072 0.916133\n",
      "          big_lips  0.504233  0.971637 0.850484\n",
      "      patchy_beard  0.506938  0.967988 0.927061\n",
      "        curly_hair  0.524519  0.988555 0.929956\n",
      "        sharp_nose  0.608675  0.933986 0.965164\n",
      "receeding_hairline  0.712881  0.953392 0.953015\n",
      "             adult  0.757033  0.920551 0.929923\n",
      "     sharp_jawline  0.776423  0.838116 0.890870\n",
      "         oily_skin  0.795454  0.824349 0.901318\n",
      "      well_groomed  0.821959  0.846077 0.919922\n"
     ]
    }
   ],
   "source": [
    "# Save results CSV\n",
    "csv_filename = f'{BACKBONE}_metrics.csv'\n",
    "csv_path = os.path.join(RESULTS_DIR, csv_filename)\n",
    "results_df.to_csv(csv_path, index=False)\n",
    "\n",
    "print(f\"\\n✓ Results saved to: {csv_path}\")\n",
    "print(f\"\\nResults preview:\")\n",
    "print(results_df.head(10))\n",
    "\n",
    "# Sort by F1 score and display top/bottom performing attributes\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TOP 10 ATTRIBUTES (by Macro F1)\")\n",
    "print(\"=\"*60)\n",
    "top_attrs = results_df.nlargest(10, 'macro_f1')[['attribute', 'macro_f1', 'accuracy', 'auc']]\n",
    "print(top_attrs.to_string(index=False))\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"BOTTOM 10 ATTRIBUTES (by Macro F1)\")\n",
    "print(\"=\"*60)\n",
    "bottom_attrs = results_df.nsmallest(10, 'macro_f1')[['attribute', 'macro_f1', 'accuracy', 'auc']]\n",
    "print(bottom_attrs.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4802bea2",
   "metadata": {},
   "source": [
    "## Training Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61f4c80a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "TRAINING COMPLETE - SUMMARY\n",
      "============================================================\n",
      "Backbone Model: convnext_tiny\n",
      "Total Parameters: 28,230,785\n",
      "Training Samples: 24112\n",
      "Validation Samples: 6029\n",
      "Number of Attributes: 33\n",
      "Number of Epochs: 1\n",
      "Best Epoch: 1\n",
      "\n",
      "Model saved at: ./Models\\convnext_tiny_best.pth\n",
      "Results saved at: ./Results\\convnext_tiny_metrics.csv\n",
      "\n",
      "============================================================\n",
      "MEAN METRICS\n",
      "============================================================\n",
      "AUC                 : 0.9576\n",
      "ACCURACY            : 0.9296\n",
      "MACRO_F1            : 0.8193\n",
      "MICRO_F1            : 0.9296\n",
      "PRECISION           : 0.8416\n",
      "RECALL              : 0.6914\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "ATTRIBUTE STATISTICS\n",
      "============================================================\n",
      "Total attributes evaluated: 33\n",
      "\n",
      "Metrics Range:\n",
      "  AUC: 0.8505 - 0.9963\n",
      "  Accuracy: 0.8243 - 0.9912\n",
      "  Macro F1: 0.4962 - 0.9744\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Final summary\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TRAINING COMPLETE - SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Backbone Model: {BACKBONE}\")\n",
    "print(f\"Total Parameters: {total_params:,}\")\n",
    "print(f\"Training Samples: {len(train_dataset)}\")\n",
    "print(f\"Validation Samples: {len(val_dataset)}\")\n",
    "print(f\"Number of Attributes: {num_attributes}\")\n",
    "print(f\"Number of Epochs: {NUM_EPOCHS}\")\n",
    "print(f\"Best Epoch: {best_epoch}\")\n",
    "print(f\"\\nModel saved at: {model_path}\")\n",
    "print(f\"Results saved at: {csv_path}\")\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"MEAN METRICS\")\n",
    "print(\"=\"*60)\n",
    "for metric, value in mean_metrics.items():\n",
    "    print(f\"{metric.upper():20s}: {value:.4f}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Display attribute distribution\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ATTRIBUTE STATISTICS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Total attributes evaluated: {len(results_df)}\")\n",
    "print(f\"\\nMetrics Range:\")\n",
    "print(f\"  AUC: {results_df['auc'].min():.4f} - {results_df['auc'].max():.4f}\")\n",
    "print(f\"  Accuracy: {results_df['accuracy'].min():.4f} - {results_df['accuracy'].max():.4f}\")\n",
    "print(f\"  Macro F1: {results_df['macro_f1'].min():.4f} - {results_df['macro_f1'].max():.4f}\")\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
